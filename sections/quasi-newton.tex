\section{Quasi-Newton Methods}

Computing and inverting the Hessian in Newton's method is costly. Quasi-Newton methods want to avoid this.

\textbf{Secant condition}: use approximation $f^\prime(x_t) \approx \frac{f(x_t) - f(x_{t-1})}{x_t - x_{t-1}}$ to find zeros. Thus, the second method updates as: $x_{t+1} = x_t - f^\prime(x_t) \frac{x_t - x_{t-1}}{f^\prime(x_t) - f^\prime(x_{t-1})}$. To generalize to higher dimensions, we want to find $H_t$ such that $\nabla f(x_t) - \nabla f(x_{t-1}) = H_t(x_t - x_{t-1})$, and do $x_{t+1} = x_t - H_t^{-1} \nabla f(x_t)$.

\textbf{Quasi-Newton method}: if the method satisfies secant condition, we say it is a quasi-Newton method. In the multidimensional case, $H_t$ is not unique. In particular, Newton's method is a quasi-Newton method iff $f$ is a non-degenerate quadratic function. For efficiency, quasi-Newton methods typically deal with the inverse directly.

\textbf{Greenstadt's approach}: update $H_t^{-1} = H_{t-1}^{-1} + E_t$ for some symmetric $E_t$ with small $\|E_t\|_F^2 = \sum_{i,j} e_{i,j}^2$. To introduce more flexibility, we minimize $\|AE_t A^\top\|_F^2$ for some fixed invertible matrix $A$. The program is: minimize $\frac{1}{2}\|AE_t A^\top\|_F^2$ such that $E_t^\top = E_t$ and $E_t (\nabla f(x_t) - \nabla f(x_{t-1})) = x_t - x_{t-1} - H_{t-1}^{-1} (\nabla f(x_t) - \nabla f(x_{t-1}))$, which is the secant condition. Note that this is a convex program, so we solve by Lagrange method.

For simplicity, we write the program as: minimize $\frac{1}{2}\|AE A^\top\|_F^2$ such that $Ey=r$ and $E^\top - E=0$. Define $f(E) = \frac{1}{2}\|AE A^\top\|_F^2$, thus $\nabla f(E) = A^\top A E A^\top A$. Note that the constraints are in fact linear over $e_{i,j}$. (11.3) Define $M = (A^\top A)^{-1}$ which is positive definite, then a solution $E^*$ is optimal iff $E^*  = M(\lambda y^\top + \Gamma^\top - \Gamma)M$, where $\lambda \in \mathbb{R}^{d\times 1}$ and $\Gamma \in \mathbb{R}^{d\times d}$. Solving the linear system gives $E^* = \frac{1}{y^\top M y}(r y^\top M + M y r^\top - \frac{y^\top r}{y^\top M y} Myy^\top M)$. This is called the Greenstadt method with parameter $M$.

\textbf{BFGS}: BFGS (named after four people) is the Greenstadt method with parameter $H_t^{-1}$. Note that $H_t^{-1}$ is not yet known in the computation of $E_t$, but we have $H_t^{-1} y = x_t - x_{t-1}$ and in the formula of $E^*$, $M$ appears in the form of $My$. This allows us to compute $E^*$ without knowing the value of $M$. BFGS ensures that if $f$ is not flat between $x_{t-1}$ and $x_t$, and $H_{t-1}$ is positive definite, then $H_{t}$ is also positive definite. This reduces per iteration cost to $O(d^2)$.

\textbf{L-BFGS}: Define $\sigma = x_t - x_{t-1}$, $y = \nabla f(x_t) - \nabla f(x_{t-1})$, $H = H_{t-1}^{-1}$ and $H^\prime = H_t^{-1}$, then BFGS can be written as $ H^\prime = (I-\frac{\sigma y^\top}{y^\top \sigma}) H (I-\frac{ y \sigma^\top}{y^\top \sigma}) + \frac{\sigma \sigma^\top}{y^\top \sigma}$. L-BFGS relies on the efficient computation of $H^\prime g^\prime$ given that $H g$ can be computed efficiently for any $g$ and $g^\prime$. This is because $H^\prime g^\prime = (I-\frac{\sigma y^\top}{y^\top \sigma}) \left[H (I-\frac{ y \sigma^\top}{y^\top \sigma}) g^\prime\right] + \sigma \frac{\sigma^\top g^\prime}{y^\top \sigma}$ and note $(I-\frac{\sigma y^\top}{y^\top \sigma}) s = s - \sigma \frac{y^\top s}{y^\top \sigma}$, thus can be computed in $O(d)$ with one call of $Hg$. Recursively, we get $O(td)$ time complexity, which is not helpful. Instead, L-BFGS only does recursion for $m$ times, and use $H_0$ instead of $H_{t-m}$ (which is unknown to us as we do not explicitly compute this) as the result of $m$-th recursion. Note that we start close, so $H_0$ is close to $H_{t-m}$. In practice, we use a better choice than $H_0$.  The final complexity is $O(md)$ per iteration.